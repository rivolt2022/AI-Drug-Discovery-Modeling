"""
MAP3K5 IC50 í™œì„±ê°’ ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ êµ¬í˜„ëœ data_loader.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í›ˆë ¨ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
CAS, ChEMBL, PubChem ë“± ëª¨ë“  ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
"""

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional, List
import logging

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.data_loader import DrugDiscoveryDataLoader, create_data_loader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MAP3K5DataPreparation:
    """
    MAP3K5 IC50 ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:
    1. ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ (ChEMBL, CAS, PubChem)
    2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì „ì²˜ë¦¬
    4. ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹ ë¶„í• 
    5. ë°ì´í„° í†µí•© ë° ì¤‘ë³µ ì œê±°
    """
    
    def __init__(self, data_dir: str = 'data', random_state: int = 42):
        """
        ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            random_state (int): ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
        """
        self.data_dir = data_dir
        self.random_state = random_state
        self.data_loader = create_data_loader(data_dir)
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.training_data = None
        self.test_data = None
        self.validation_data = None
        
        logger.info(f"MAP3K5 ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir})")
    
    def load_cas_data_directly(self) -> Optional[pd.DataFrame]:
        """
        CAS ë°ì´í„°ë¥¼ ì§ì ‘ ë¡œë“œí•˜ì—¬ ì²˜ë¦¬
        
        Returns:
            pd.DataFrame: ì²˜ë¦¬ëœ CAS ë°ì´í„°
        """
        logger.info("CAS ë°ì´í„° ì§ì ‘ ë¡œë”© ì‹œì‘...")
        
        try:
            # CAS ì—‘ì…€ íŒŒì¼ì˜ IC50 ì‹œíŠ¸ ì§ì ‘ ì½ê¸°
            from data.preprocessing.read_excel import read_ic50_sheet
            
            cas_df = read_ic50_sheet()
            
            if cas_df is None:
                logger.warning("CAS ì—‘ì…€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨")
                return None
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['SMILES', 'Single Value (Parsed)', 'pX Value']
            available_cols = [col for col in required_cols if col in cas_df.columns]
            
            if len(available_cols) < 2:
                logger.warning(f"CAS ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {available_cols}")
                return None
            
            # ë°ì´í„° ì •ì œ
            training_df = cas_df[available_cols].copy()
            
            # SMILES ê²°ì¸¡ì¹˜ ì œê±°
            training_df = training_df.dropna(subset=['SMILES'])
            
            # IC50 ê°’ ì²˜ë¦¬
            if 'Single Value (Parsed)' in training_df.columns:
                # Single Value (Parsed)ë¥¼ IC50_nMìœ¼ë¡œ ë³€í™˜
                training_df['IC50_nM'] = pd.to_numeric(training_df['Single Value (Parsed)'], errors='coerce')
                training_df = training_df.dropna(subset=['IC50_nM'])
                
                # pIC50 ê³„ì‚° (Single Valueê°€ ì´ë¯¸ nM ë‹¨ìœ„ë¼ê³  ê°€ì •)
                training_df['pIC50'] = -np.log10(training_df['IC50_nM'] * 1e-9)
            
            elif 'pX Value' in training_df.columns:
                # pX Valueê°€ ì´ë¯¸ pIC50 ê°’ì¸ ê²½ìš°
                training_df['pIC50'] = pd.to_numeric(training_df['pX Value'], errors='coerce')
                training_df = training_df.dropna(subset=['pIC50'])
                
                # IC50_nM ê³„ì‚°
                training_df['IC50_nM'] = 10**(-training_df['pIC50']) * 1e9
            
            # ìµœì¢… ì»¬ëŸ¼ë§Œ ì„ íƒ
            final_df = training_df[['SMILES', 'IC50_nM', 'pIC50']].copy()
            
            # ì¤‘ë³µ SMILES ì œê±° (ê°€ì¥ ë†’ì€ pIC50 ê°’ ìœ ì§€)
            final_df = final_df.sort_values('pIC50', ascending=False)
            final_df = final_df.drop_duplicates(subset=['SMILES'], keep='first')
            
            logger.info(f"âœ… CAS ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(final_df)}ê°œ í™”í•©ë¬¼")
            logger.info(f"   - pIC50 ë²”ìœ„: {final_df['pIC50'].min():.2f} ~ {final_df['pIC50'].max():.2f}")
            logger.info(f"   - IC50 ë²”ìœ„: {final_df['IC50_nM'].min():.2e} ~ {final_df['IC50_nM'].max():.2e} nM")
            
            return final_df
            
        except Exception as e:
            logger.error(f"CAS ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_pubchem_data(self) -> Optional[pd.DataFrame]:
        """
        PubChem ë°ì´í„° ë¡œë“œ
        
        Returns:
            pd.DataFrame: ì²˜ë¦¬ëœ PubChem ë°ì´í„°
        """
        logger.info("PubChem ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        try:
            # PubChem CSV íŒŒì¼ ì½ê¸°
            pubchem_file = os.path.join(self.data_dir, 'Pubchem_ASK1.csv')
            
            if not os.path.exists(pubchem_file):
                logger.warning(f"PubChem íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pubchem_file}")
                return None
            
            # CSV íŒŒì¼ ì½ê¸° (low_memory=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²½ê³  ì œê±°)
            df = pd.read_csv(pubchem_file, low_memory=False)
            
            logger.info(f"ğŸ“Š PubChem ë°ì´í„° í¬ê¸°: {df.shape}")
            logger.info(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
            
            # IC50 íƒ€ì…ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
            if 'Activity_Type' in df.columns:
                ic50_data = df[df['Activity_Type'] == 'IC50'].copy()
                logger.info(f"ğŸ“Š IC50 íƒ€ì… ë°ì´í„°: {len(ic50_data)}ê°œ")
            else:
                ic50_data = df.copy()
                logger.info(f"ğŸ“Š Activity_Type ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(ic50_data)}ê°œ")
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
            required_cols = ['SMILES', 'Activity_Value']
            available_cols = [col for col in required_cols if col in ic50_data.columns]
            
            if len(available_cols) < 2:
                logger.warning(f"PubChem ë°ì´í„°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {available_cols}")
                logger.warning(f"ì „ì²´ ì»¬ëŸ¼: {list(ic50_data.columns)}")
                return None
            
            # ë°ì´í„° ì •ì œ
            training_df = ic50_data[available_cols].copy()
            training_df = training_df.rename(columns={'Activity_Value': 'IC50_raw'})
            
            # ê²°ì¸¡ì¹˜ ì œê±°
            training_df = training_df.dropna()
            
            # IC50 ê°’ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
            training_df['IC50_nM'] = pd.to_numeric(training_df['IC50_raw'], errors='coerce')
            training_df = training_df.dropna(subset=['IC50_nM'])
            
            # IC50 ê°’ì´ ì–‘ìˆ˜ì¸ì§€ í™•ì¸
            training_df = training_df[training_df['IC50_nM'] > 0]
            
            # pIC50 ê³„ì‚° (Activity_Valueê°€ ì´ë¯¸ Î¼M ë‹¨ìœ„ë¼ê³  ê°€ì •)
            training_df['pIC50'] = -np.log10(training_df['IC50_nM'] * 1e-6)
            
            # ìµœì¢… ì»¬ëŸ¼ë§Œ ì„ íƒ
            final_df = training_df[['SMILES', 'IC50_nM', 'pIC50']].copy()
            
            # ì¤‘ë³µ SMILES ì œê±° (ê°€ì¥ ë†’ì€ pIC50 ê°’ ìœ ì§€)
            final_df = final_df.sort_values('pIC50', ascending=False)
            final_df = final_df.drop_duplicates(subset=['SMILES'], keep='first')
            
            logger.info(f"âœ… PubChem ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(final_df)}ê°œ í™”í•©ë¬¼")
            logger.info(f"   - pIC50 ë²”ìœ„: {final_df['pIC50'].min():.2f} ~ {final_df['pIC50'].max():.2f}")
            logger.info(f"   - IC50 ë²”ìœ„: {final_df['IC50_nM'].min():.2e} ~ {final_df['IC50_nM'].max():.2e} Î¼M")
            
            return final_df
            
        except Exception as e:
            logger.error(f"PubChem ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def load_all_data(self) -> Dict[str, pd.DataFrame]:
        """
        ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë°˜í™˜
        
        Returns:
            Dict[str, pd.DataFrame]: ë¡œë“œëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        logger.info("ëª¨ë“  ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        data_dict = {}
        
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        test_data = self.data_loader.load_test_data()
        if test_data is not None:
            data_dict['test'] = test_data
            self.test_data = test_data
            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_data)}ê°œ í™”í•©ë¬¼")
        else:
            logger.warning("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 2. ChEMBL ë°ì´í„° ë¡œë“œ
        logger.info("ChEMBL ë°ì´í„° ë¡œë”© ì¤‘...")
        chembl_data = self.data_loader.load_training_data_from_chembl()
        if chembl_data is not None:
            data_dict['chembl'] = chembl_data
            logger.info(f"âœ… ChEMBL ë°ì´í„°: {len(chembl_data)}ê°œ í™”í•©ë¬¼")
        else:
            logger.warning("âŒ ChEMBL ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 3. CAS ë°ì´í„° ì§ì ‘ ë¡œë“œ
        logger.info("CAS ë°ì´í„° ì§ì ‘ ë¡œë”© ì¤‘...")
        cas_data = self.load_cas_data_directly()
        if cas_data is not None:
            data_dict['cas'] = cas_data
            logger.info(f"âœ… CAS ë°ì´í„°: {len(cas_data)}ê°œ í™”í•©ë¬¼")
        else:
            logger.warning("âŒ CAS ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 4. PubChem ë°ì´í„° ë¡œë“œ
        logger.info("PubChem ë°ì´í„° ë¡œë”© ì¤‘...")
        pubchem_data = self.load_pubchem_data()
        if pubchem_data is not None:
            data_dict['pubchem'] = pubchem_data
            logger.info(f"âœ… PubChem ë°ì´í„°: {len(pubchem_data)}ê°œ í™”í•©ë¬¼")
        else:
            logger.warning("âŒ PubChem ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        
        # 5. ê²°í•©ëœ í›ˆë ¨ ë°ì´í„° ìƒì„±
        logger.info("ê²°í•©ëœ í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘...")
        combined_data = self.combine_all_training_data(data_dict)
        if combined_data is not None:
            data_dict['combined'] = combined_data
            self.training_data = combined_data
            logger.info(f"âœ… ê²°í•©ëœ í›ˆë ¨ ë°ì´í„°: {len(combined_data)}ê°œ í™”í•©ë¬¼")
        
        return data_dict
    
    def combine_all_training_data(self, data_dict: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
        """
        ëª¨ë“  í›ˆë ¨ ë°ì´í„°ë¥¼ ê²°í•©
        
        Args:
            data_dict: ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            pd.DataFrame: ê²°í•©ëœ í›ˆë ¨ ë°ì´í„°
        """
        datasets = []
        
        # ê° ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘
        for source, data in data_dict.items():
            if source in ['chembl', 'cas', 'pubchem'] and data is not None:
                # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
                data_with_source = data.copy()
                data_with_source['source'] = source
                datasets.append(data_with_source)
                logger.info(f"  - {source.upper()}: {len(data)}ê°œ í™”í•©ë¬¼")
        
        if not datasets:
            logger.error("ê²°í•©í•  í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë°ì´í„° ê²°í•©
        combined_df = pd.concat(datasets, ignore_index=True)
        
        # ì¤‘ë³µ SMILES ì œê±° (ê°€ì¥ ë†’ì€ pIC50 ê°’ ìœ ì§€)
        original_count = len(combined_df)
        combined_df = combined_df.sort_values('pIC50', ascending=False)
        combined_df = combined_df.drop_duplicates(subset=['SMILES'], keep='first')
        final_count = len(combined_df)
        
        logger.info(f"ğŸ“Š ë°ì´í„° ê²°í•© ê²°ê³¼:")
        logger.info(f"   - ì›ë³¸ ì´ í™”í•©ë¬¼: {original_count}ê°œ")
        logger.info(f"   - ì¤‘ë³µ ì œê±° í›„: {final_count}ê°œ")
        logger.info(f"   - ì œê±°ëœ ì¤‘ë³µ: {original_count - final_count}ê°œ")
        
        # ì†ŒìŠ¤ë³„ ë¶„í¬
        source_dist = combined_df['source'].value_counts()
        logger.info(f"   - ì†ŒìŠ¤ë³„ ë¶„í¬:")
        for source, count in source_dist.items():
            logger.info(f"     {source.upper()}: {count}ê°œ")
        
        return combined_df
    
    def validate_data_quality(self, data: pd.DataFrame, data_type: str = "training") -> Dict[str, any]:
        """
        ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        
        Args:
            data (pd.DataFrame): ê²€ì¦í•  ë°ì´í„°
            data_type (str): ë°ì´í„° íƒ€ì… ("training", "test")
            
        Returns:
            Dict[str, any]: ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"{data_type} ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        validation_results = {
            'total_count': len(data),
            'missing_values': {},
            'duplicates': 0,
            'smiles_validation': {},
            'ic50_stats': {}
        }
        
        # 1. ê²°ì¸¡ì¹˜ ê²€ì‚¬
        missing_counts = data.isnull().sum()
        validation_results['missing_values'] = missing_counts.to_dict()
        
        # 2. ì¤‘ë³µ ê²€ì‚¬
        if 'SMILES' in data.columns:
            duplicates = data.duplicated(subset=['SMILES']).sum()
            validation_results['duplicates'] = duplicates
        
        # 3. SMILES ìœ íš¨ì„± ê²€ì‚¬
        if 'SMILES' in data.columns:
            # SMILES ê¸¸ì´ ë¶„í¬
            smiles_lengths = data['SMILES'].str.len()
            validation_results['smiles_validation'] = {
                'min_length': smiles_lengths.min(),
                'max_length': smiles_lengths.max(),
                'mean_length': smiles_lengths.mean(),
                'empty_smiles': (data['SMILES'] == '').sum(),
                'whitespace_only': (data['SMILES'].str.strip() == '').sum()
            }
        
        # 4. IC50 í†µê³„ (í›ˆë ¨ ë°ì´í„°ì¸ ê²½ìš°)
        if data_type == "training" and 'pIC50' in data.columns:
            validation_results['ic50_stats'] = {
                'min': data['pIC50'].min(),
                'max': data['pIC50'].max(),
                'mean': data['pIC50'].mean(),
                'median': data['pIC50'].median(),
                'std': data['pIC50'].std()
            }
        
        # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
        logger.info(f"ğŸ“Š {data_type} ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
        logger.info(f"   - ì´ í™”í•©ë¬¼ ìˆ˜: {validation_results['total_count']}")
        logger.info(f"   - ì¤‘ë³µ í™”í•©ë¬¼: {validation_results['duplicates']}")
        
        if validation_results['missing_values']:
            logger.info(f"   - ê²°ì¸¡ì¹˜: {validation_results['missing_values']}")
        
        if validation_results['smiles_validation']:
            smiles_info = validation_results['smiles_validation']
            logger.info(f"   - SMILES ê¸¸ì´: {smiles_info['min_length']}~{smiles_info['max_length']} (í‰ê· : {smiles_info['mean_length']:.1f})")
        
        if validation_results['ic50_stats']:
            ic50_info = validation_results['ic50_stats']
            logger.info(f"   - pIC50 ë²”ìœ„: {ic50_info['min']:.2f}~{ic50_info['max']:.2f} (í‰ê· : {ic50_info['mean']:.2f})")
        
        return validation_results
    
    def prepare_model_datasets(self, test_size: float = 0.2, validation_size: float = 0.2) -> Dict[str, pd.DataFrame]:
        """
        ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„
        
        Args:
            test_size (float): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨
            validation_size (float): ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨
            
        Returns:
            Dict[str, pd.DataFrame]: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
        """
        logger.info("ëª¨ë¸ í›ˆë ¨ìš© ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        if self.training_data is None:
            all_data = self.load_all_data()
            if 'combined' in all_data:
                self.training_data = all_data['combined']
        
        if self.test_data is None:
            self.test_data = self.data_loader.load_test_data()
        
        if self.training_data is None:
            logger.error("í›ˆë ¨ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        training_validation = self.validate_data_quality(self.training_data, "training")
        
        # ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ (data_loaderì˜ prepare_model_data ì‚¬ìš©)
        model_data = self.data_loader.prepare_model_data(
            test_size=validation_size,  # validation_sizeë¥¼ test_sizeë¡œ ì‚¬ìš©
            random_state=self.random_state
        )
        
        if model_data is None:
            logger.error("ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {}
        
        # ê²°ê³¼ ì •ë¦¬
        datasets = {
            'train': model_data['train'],
            'validation': model_data['validation'],
            'test': self.test_data,  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
            'full_training': model_data['full_training']
        }
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        logger.info("ğŸ“Š ëª¨ë¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ:")
        logger.info(f"   - í›ˆë ¨ ì„¸íŠ¸: {len(datasets['train'])}ê°œ í™”í•©ë¬¼")
        logger.info(f"   - ê²€ì¦ ì„¸íŠ¸: {len(datasets['validation'])}ê°œ í™”í•©ë¬¼")
        logger.info(f"   - í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(datasets['test'])}ê°œ í™”í•©ë¬¼")
        logger.info(f"   - ì „ì²´ í›ˆë ¨: {len(datasets['full_training'])}ê°œ í™”í•©ë¬¼")
        
        return datasets
    
    def get_data_summary(self) -> Dict[str, any]:
        """
        ì „ì²´ ë°ì´í„° ìš”ì•½ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, any]: ë°ì´í„° ìš”ì•½ ì •ë³´
        """
        logger.info("ë°ì´í„° ìš”ì•½ ì •ë³´ ìƒì„± ì¤‘...")
        
        summary = {}
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìš”ì•½
        if self.test_data is not None:
            summary['test'] = {
                'count': len(self.test_data),
                'columns': list(self.test_data.columns)
            }
        
        # í›ˆë ¨ ë°ì´í„° ìš”ì•½
        if self.training_data is not None:
            summary['training'] = {
                'count': len(self.training_data),
                'columns': list(self.training_data.columns),
                'pIC50_stats': {
                    'min': self.training_data['pIC50'].min(),
                    'max': self.training_data['pIC50'].max(),
                    'mean': self.training_data['pIC50'].mean(),
                    'median': self.training_data['pIC50'].median()
                },
                'source_distribution': self.training_data['source'].value_counts().to_dict()
            }
        
        # ì¶”ê°€ ì •ë³´
        if self.training_data is not None:
            summary['training_quality'] = self.validate_data_quality(self.training_data, "training")
        
        if self.test_data is not None:
            summary['test_quality'] = self.validate_data_quality(self.test_data, "test")
        
        return summary
    
    def save_prepared_data(self, output_dir: str = "prepared_data") -> None:
        """
        ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        logger.info(f"ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ {output_dir}ì— ì €ì¥ ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        datasets = self.prepare_model_datasets()
        
        # ê° ë°ì´í„°ì…‹ ì €ì¥
        for name, data in datasets.items():
            if data is not None and len(data) > 0:
                output_path = os.path.join(output_dir, f"{name}_data.csv")
                data.to_csv(output_path, index=False)
                logger.info(f"âœ… {name} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path} ({len(data)}ê°œ í™”í•©ë¬¼)")
        
        # ë°ì´í„° ìš”ì•½ ì €ì¥
        summary = self.get_data_summary()
        summary_path = os.path.join(output_dir, "data_summary.txt")
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("MAP3K5 IC50 ì˜ˆì¸¡ ëª¨ë¸ ë°ì´í„° ìš”ì•½\n")
            f.write("=" * 50 + "\n\n")
            
            for key, value in summary.items():
                f.write(f"{key}:\n")
                f.write(str(value))
                f.write("\n\n")
        
        logger.info(f"âœ… ë°ì´í„° ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("MAP3K5 ë°ì´í„° ì¤€ë¹„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    
    # ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤ ì´ˆê¸°í™”
    data_prep = MAP3K5DataPreparation()
    
    # ëª¨ë“  ë°ì´í„° ë¡œë“œ
    all_data = data_prep.load_all_data()
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    for data_name, data in all_data.items():
        if data is not None:
            data_prep.validate_data_quality(data, data_name)
    
    # ëª¨ë¸ ë°ì´í„°ì…‹ ì¤€ë¹„
    model_datasets = data_prep.prepare_model_datasets()
    
    # ë°ì´í„° ì €ì¥
    data_prep.save_prepared_data()
    
    # ìµœì¢… ìš”ì•½ ì¶œë ¥
    summary = data_prep.get_data_summary()
    logger.info("ğŸ‰ MAP3K5 ë°ì´í„° ì¤€ë¹„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    
    return model_datasets


if __name__ == "__main__":
    main() 