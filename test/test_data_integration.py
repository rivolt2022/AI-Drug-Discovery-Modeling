"""
ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ í†µí•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data_loader import create_data_loader

def test_data_quality():
    """ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    loader = create_data_loader()
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    print("\n1ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬")
    test_data = loader.load_test_data()
    
    if test_data is not None:
        print(f"   - ì´ í™”í•©ë¬¼ ìˆ˜: {len(test_data)}")
        print(f"   - ê³ ìœ  SMILES ìˆ˜: {test_data['SMILES'].nunique()}")
        print(f"   - ì¤‘ë³µ SMILES ìˆ˜: {len(test_data) - test_data['SMILES'].nunique()}")
        
        # SMILES ê¸¸ì´ ë¶„í¬
        smiles_lengths = test_data['SMILES'].str.len()
        print(f"   - SMILES í‰ê·  ê¸¸ì´: {smiles_lengths.mean():.1f}")
        print(f"   - SMILES ìµœì†Œ ê¸¸ì´: {smiles_lengths.min()}")
        print(f"   - SMILES ìµœëŒ€ ê¸¸ì´: {smiles_lengths.max()}")
        
        # ID íŒ¨í„´ ë¶„ì„
        id_patterns = test_data['ID'].str.extract(r'([A-Z]+)_(\d+)')
        print(f"   - ID íŒ¨í„´: {id_patterns[0].iloc[0]}_XXXX")
    
    # 2. í›ˆë ¨ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
    print("\n2ï¸âƒ£ í›ˆë ¨ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬")
    training_data = loader.load_combined_training_data()
    
    if training_data is not None:
        print(f"   - ì´ í™”í•©ë¬¼ ìˆ˜: {len(training_data)}")
        print(f"   - ê³ ìœ  SMILES ìˆ˜: {training_data['SMILES'].nunique()}")
        print(f"   - ì¤‘ë³µ SMILES ìˆ˜: {len(training_data) - training_data['SMILES'].nunique()}")
        
        # pIC50 ë¶„í¬ ë¶„ì„
        pic50_stats = training_data['pIC50'].describe()
        print(f"   - pIC50 í†µê³„:")
        print(f"     * í‰ê· : {pic50_stats['mean']:.2f}")
        print(f"     * í‘œì¤€í¸ì°¨: {pic50_stats['std']:.2f}")
        print(f"     * ìµœì†Œê°’: {pic50_stats['min']:.2f}")
        print(f"     * ìµœëŒ€ê°’: {pic50_stats['max']:.2f}")
        print(f"     * 25%: {pic50_stats['25%']:.2f}")
        print(f"     * 50%: {pic50_stats['50%']:.2f}")
        print(f"     * 75%: {pic50_stats['75%']:.2f}")
        
        # ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬
        source_dist = training_data['source'].value_counts()
        print(f"   - ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬:")
        for source, count in source_dist.items():
            print(f"     * {source}: {count}ê°œ ({count/len(training_data)*100:.1f}%)")
        
        # IC50 ë¶„í¬ ë¶„ì„
        ic50_stats = training_data['IC50_nM'].describe()
        print(f"   - IC50 í†µê³„ (nM):")
        print(f"     * í‰ê· : {ic50_stats['mean']:.2f}")
        print(f"     * ì¤‘ì•™ê°’: {ic50_stats['50%']:.2f}")
        print(f"     * ìµœì†Œê°’: {ic50_stats['min']:.2f}")
        print(f"     * ìµœëŒ€ê°’: {ic50_stats['max']:.2f}")
    
    print("\nâœ… ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

def test_data_distribution():
    """ë°ì´í„° ë¶„í¬ ì‹œê°í™” ë° ë¶„ì„"""
    print("\nğŸ“Š ë°ì´í„° ë¶„í¬ ë¶„ì„")
    print("=" * 40)
    
    loader = create_data_loader()
    training_data = loader.load_combined_training_data()
    
    if training_data is None:
        print("âŒ í›ˆë ¨ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 1. pIC50 ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.hist(training_data['pIC50'], bins=30, alpha=0.7, edgecolor='black')
    plt.title('pIC50 ë¶„í¬')
    plt.xlabel('pIC50')
    plt.ylabel('ë¹ˆë„')
    plt.grid(True, alpha=0.3)
    
    # 2. IC50 ë¶„í¬ (ë¡œê·¸ ìŠ¤ì¼€ì¼)
    plt.subplot(2, 2, 2)
    plt.hist(np.log10(training_data['IC50_nM']), bins=30, alpha=0.7, edgecolor='black')
    plt.title('IC50 ë¶„í¬ (ë¡œê·¸ ìŠ¤ì¼€ì¼)')
    plt.xlabel('log10(IC50_nM)')
    plt.ylabel('ë¹ˆë„')
    plt.grid(True, alpha=0.3)
    
    # 3. ë°ì´í„° ì†ŒìŠ¤ë³„ pIC50 ë¶„í¬
    plt.subplot(2, 2, 3)
    sources = training_data['source'].unique()
    for source in sources:
        source_data = training_data[training_data['source'] == source]['pIC50']
        plt.hist(source_data, bins=20, alpha=0.6, label=source)
    plt.title('ë°ì´í„° ì†ŒìŠ¤ë³„ pIC50 ë¶„í¬')
    plt.xlabel('pIC50')
    plt.ylabel('ë¹ˆë„')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. SMILES ê¸¸ì´ ë¶„í¬
    plt.subplot(2, 2, 4)
    smiles_lengths = training_data['SMILES'].str.len()
    plt.hist(smiles_lengths, bins=30, alpha=0.7, edgecolor='black')
    plt.title('SMILES ê¸¸ì´ ë¶„í¬')
    plt.xlabel('SMILES ê¸¸ì´')
    plt.ylabel('ë¹ˆë„')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ê²°ê³¼ ì €ì¥
    output_dir = os.path.join(os.path.dirname(__file__), 'output')
    os.makedirs(output_dir, exist_ok=True)
    plt.savefig(os.path.join(output_dir, 'data_distribution.png'), dpi=300, bbox_inches='tight')
    plt.show()
    
    print("âœ… ë°ì´í„° ë¶„í¬ ë¶„ì„ ì™„ë£Œ")

def test_data_preprocessing():
    """ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    loader = create_data_loader()
    
    # 1. SMILES ìœ íš¨ì„± ê²€ì‚¬ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ SMILES ìœ íš¨ì„± ê²€ì‚¬")
    
    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    test_smiles = [
        'CC(=O)OC1=CC=CC=C1C(=O)O',  # ì•„ìŠ¤í”¼ë¦° - ìœ íš¨
        'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # ì´ë¶€í”„ë¡œíœ - ìœ íš¨
        '',  # ë¹ˆ ë¬¸ìì—´
        'CC',  # ë„ˆë¬´ ì§§ìŒ
        'INVALID_SMILES_STRING',  # ìœ íš¨í•˜ì§€ ì•ŠìŒ
        '   ',  # ê³µë°±ë§Œ
        'CC(=O)OC1=CC=CC=C1C(=O)O'  # ì¤‘ë³µ
    ]
    
    test_df = pd.DataFrame({
        'SMILES': test_smiles,
        'pIC50': [5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0]
    })
    
    print(f"   - ì›ë³¸ ë°ì´í„°: {len(test_df)}ê°œ")
    
    # ìœ íš¨ì„± ê²€ì‚¬ ì‹¤í–‰
    validated_df = loader.validate_smiles(test_df)
    
    print(f"   - ê²€ì¦ í›„ ë°ì´í„°: {len(validated_df)}ê°œ")
    print(f"   - ì œê±°ëœ ë°ì´í„°: {len(test_df) - len(validated_df)}ê°œ")
    
    # 2. ëª¨ë¸ ë°ì´í„° ì¤€ë¹„ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ëª¨ë¸ ë°ì´í„° ì¤€ë¹„")
    
    model_data = loader.prepare_model_data(test_size=0.2, random_state=42)
    
    if model_data:
        print(f"   - í›ˆë ¨ ì„¸íŠ¸: {len(model_data['train'])}ê°œ")
        print(f"   - ê²€ì¦ ì„¸íŠ¸: {len(model_data['validation'])}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(model_data['test'])}ê°œ")
        
        # ë¶„í•  ë¹„ìœ¨ í™•ì¸
        total_train = len(model_data['train']) + len(model_data['validation'])
        val_ratio = len(model_data['validation']) / total_train
        print(f"   - ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨: {val_ratio:.2f} (ëª©í‘œ: 0.20)")
        
        # ë°ì´í„° ì¤‘ë³µ í™•ì¸
        train_smiles = set(model_data['train']['SMILES'])
        val_smiles = set(model_data['validation']['SMILES'])
        overlap = train_smiles.intersection(val_smiles)
        print(f"   - í›ˆë ¨/ê²€ì¦ ì¤‘ë³µ: {len(overlap)}ê°œ")
        
        if len(overlap) == 0:
            print("   âœ… í›ˆë ¨/ê²€ì¦ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("   âš ï¸ í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì— ì¤‘ë³µì´ ìˆìŠµë‹ˆë‹¤.")
    
    print("\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

def test_performance():
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nâš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    import time
    
    loader = create_data_loader()
    
    # 1. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    # ì²« ë²ˆì§¸ ë¡œë“œ (ìºì‹œ ì—†ìŒ)
    start_time = time.time()
    test_data_1 = loader.load_test_data(use_cache=False)
    first_load_time = time.time() - start_time
    
    # ë‘ ë²ˆì§¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)
    start_time = time.time()
    test_data_2 = loader.load_test_data(use_cache=True)
    cached_load_time = time.time() - start_time
    
    print(f"   - ì²« ë²ˆì§¸ ë¡œë“œ ì‹œê°„: {first_load_time:.3f}ì´ˆ")
    print(f"   - ìºì‹œ ì‚¬ìš© ë¡œë“œ ì‹œê°„: {cached_load_time:.3f}ì´ˆ")
    print(f"   - ì„±ëŠ¥ í–¥ìƒ: {first_load_time/cached_load_time:.1f}ë°°")
    
    # 2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    
    start_time = time.time()
    training_data = loader.load_combined_training_data(use_cache=False)
    load_time = time.time() - start_time
    
    if training_data is not None:
        print(f"   - ë°ì´í„° í¬ê¸°: {len(training_data)}ê°œ í™”í•©ë¬¼")
        print(f"   - ë¡œë“œ ì‹œê°„: {load_time:.3f}ì´ˆ")
        print(f"   - ì²˜ë¦¬ ì†ë„: {len(training_data)/load_time:.0f} í™”í•©ë¬¼/ì´ˆ")
    
    print("\nâœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

def generate_test_report():
    """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±")
    print("=" * 40)
    
    loader = create_data_loader()
    
    # ë°ì´í„° ìš”ì•½ ìˆ˜ì§‘
    summary = loader.get_data_summary()
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±
    output_dir = os.path.join(os.path.dirname(__file__), 'output')
    os.makedirs(output_dir, exist_ok=True)
    
    report_file = os.path.join(output_dir, 'test_report.txt')
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("AI ì‹ ì•½ê°œë°œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸\n")
        f.write("=" * 50 + "\n\n")
        
        f.write("1. ë°ì´í„° ìš”ì•½\n")
        f.write("-" * 20 + "\n")
        
        for key, value in summary.items():
            f.write(f"\n{key.upper()} ë°ì´í„°:\n")
            if key == 'test':
                f.write(f"  - í™”í•©ë¬¼ ìˆ˜: {value['count']}\n")
                f.write(f"  - ì»¬ëŸ¼: {', '.join(value['columns'])}\n")
            elif key == 'training':
                f.write(f"  - í™”í•©ë¬¼ ìˆ˜: {value['count']}\n")
                f.write(f"  - ì»¬ëŸ¼: {', '.join(value['columns'])}\n")
                
                stats = value['pIC50_stats']
                f.write(f"  - pIC50 í†µê³„:\n")
                f.write(f"    * ìµœì†Œê°’: {stats['min']:.2f}\n")
                f.write(f"    * ìµœëŒ€ê°’: {stats['max']:.2f}\n")
                f.write(f"    * í‰ê· ê°’: {stats['mean']:.2f}\n")
                f.write(f"    * ì¤‘ì•™ê°’: {stats['median']:.2f}\n")
                
                f.write(f"  - ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬:\n")
                for source, count in value['source_distribution'].items():
                    f.write(f"    * {source}: {count}ê°œ\n")
        
        f.write("\n2. í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
        f.write("-" * 20 + "\n")
        f.write("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n")
        f.write("âœ… ë°ì´í„° ë¡œë”© ê¸°ëŠ¥ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.\n")
        f.write("âœ… ë°ì´í„° í’ˆì§ˆì´ ëª¨ë¸ í•™ìŠµì— ì í•©í•©ë‹ˆë‹¤.\n")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ë°ì´í„° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # 1. ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        test_data_quality()
        
        # 2. ë°ì´í„° ë¶„í¬ ë¶„ì„
        test_data_distribution()
        
        # 3. ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        test_data_preprocessing()
        
        # 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_performance()
        
        # 5. í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
        generate_test_report()
        
        print("\nï¿½ï¿½ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
